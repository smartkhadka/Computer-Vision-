{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbqjLo/XKayyhsuhZInEO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartkhadka/Computer-Vision-/blob/main/Customdatasetipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWDfLK_jksAf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Example transforms\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Usage\n",
        "dataset_cls = ClassificationDataset(image_paths, labels, transform=img_transform)"
      ],
      "metadata": {
        "id": "ZAyuETXgk0mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, transform_img=None, transform_mask=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_mask = transform_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
        "\n",
        "        if self.transform_img:\n",
        "            image = self.transform_img(image)\n",
        "        if self.transform_mask:\n",
        "            mask = self.transform_mask(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Example transforms\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256), interpolation=Image.BILINEAR),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((256,256), interpolation=Image.NEAREST),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Usage\n",
        "dataset_seg = SegmentationDataset(image_paths, mask_paths,\n",
        "                                  transform_img=img_transform,\n",
        "                                  transform_mask=mask_transform)\n"
      ],
      "metadata": {
        "id": "CQfMxGGwlDhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class DetectionDataset(Dataset):\n",
        "    def __init__(self, image_paths, annotations, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.annotations = annotations  # list of dicts {\"boxes\":..., \"labels\":...}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        target = self.annotations[idx]  # dict with \"boxes\" and \"labels\"\n",
        "\n",
        "        if self.transform:\n",
        "            image, target = self.transform(image, target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "# Example transform (using torchvision detection API style)\n",
        "import torchvision.transforms as T\n",
        "\n",
        "def detection_transform(image, target):\n",
        "    transform = T.Compose([\n",
        "        T.Resize((300,300)),\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "    image = transform(image)\n",
        "    # target stays as dict, but you may need to scale boxes if resizing\n",
        "    return image, target\n",
        "\n",
        "# Usage\n",
        "dataset_det = DetectionDataset(image_paths, annotations, transform=detection_transform)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "yaTxw952lHrg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}